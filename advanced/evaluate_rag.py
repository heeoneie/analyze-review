"""
Level 4-2: RAG ì‹œìŠ¤í…œ í‰ê°€
Ground Truth ë°ì´í„°ë¡œ RAG ê¸°ë°˜ ë¶„ë¥˜ ì •í™•ë„ ì¸¡ì •
"""

import json
import os
import sys
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from advanced.rag_system import RAGReviewAnalyzer


def evaluate_rag(ground_truth_file: str, n_examples: int = 3):
    """RAG ì‹œìŠ¤í…œ í‰ê°€"""
    print("=" * 80)
    print("  RAG ê¸°ë°˜ ë¦¬ë·° ë¶„ë¥˜ í‰ê°€")
    print("=" * 80)

    # Ground Truth ë¡œë“œ
    print(f"\nğŸ“‚ Ground Truth ë¡œë”©: {ground_truth_file}")
    df = pd.read_csv(ground_truth_file)
    df = df[df['manual_label'].notna()]
    print(f"   âœ“ {len(df)}ê°œ ë¦¬ë·° ë¡œë“œ")

    # RAG ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë“œ
    print("\nğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
    analyzer = RAGReviewAnalyzer()
    analyzer.load_ground_truth(ground_truth_file)

    # ì˜ˆì¸¡ ì‹¤í–‰ (Leave-one-out ë°©ì‹)
    print(f"\nğŸ¤– RAG ê¸°ë°˜ ë¶„ë¥˜ ì‹œì‘ (ê²€ìƒ‰ ì˜ˆì‹œ: {n_examples}ê°œ)...")

    reviews = df['review_text'].tolist()
    y_true = df['manual_label'].tolist()
    y_pred = []

    for i, review in enumerate(reviews):
        print(f"   [{i+1}/{len(reviews)}] ë¶„ì„ ì¤‘...", end='\r')
        try:
            result = analyzer.categorize_with_rag(review, n_examples=n_examples)
            y_pred.append(result['category'])
        except Exception as e:
            print(f"\n   âš ï¸ ì—ëŸ¬ (Review {i+1}): {e}")
            y_pred.append('other')

    print("\n   âœ“ ì™„ë£Œ!")

    # í‰ê°€
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='weighted', zero_division=0
    )

    # ì—ëŸ¬ ë¶„ì„
    errors = []
    for i, (true, pred) in enumerate(zip(y_true, y_pred)):
        if true != pred:
            errors.append({
                'review_id': int(df.iloc[i]['review_id']),
                'review_text': reviews[i][:100] + '...',
                'true_label': true,
                'predicted_label': pred
            })

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("  í‰ê°€ ê²°ê³¼")
    print("=" * 80)
    print(f"\nğŸ“Š RAG ê¸°ë°˜ ë¶„ë¥˜ (ê²€ìƒ‰ ì˜ˆì‹œ: {n_examples}ê°œ)")
    print(f"   Accuracy:  {accuracy * 100:.2f}%")
    print(f"   Precision: {precision * 100:.2f}%")
    print(f"   Recall:    {recall * 100:.2f}%")
    print(f"   F1 Score:  {f1 * 100:.2f}%")
    print(f"\n   ì—ëŸ¬: {len(errors)}ê°œ / {len(y_true)}ê°œ")

    # ê²°ê³¼ ì €ì¥
    os.makedirs('results', exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    results = {
        'method': f'RAG (n_examples={n_examples})',
        'accuracy': round(accuracy, 4),
        'precision': round(precision, 4),
        'recall': round(recall, 4),
        'f1': round(f1, 4),
        'total_samples': len(y_true),
        'errors': len(errors),
        'error_details': errors[:10],  # ì²˜ìŒ 10ê°œë§Œ
        'timestamp': timestamp
    }

    output_file = f'results/rag_evaluation_{timestamp}.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='RAG ì‹œìŠ¤í…œ í‰ê°€')
    parser.add_argument('--ground-truth', type=str,
                        default='evaluation/evaluation_dataset.csv',
                        help='Ground Truth CSV íŒŒì¼')
    parser.add_argument('--n-examples', type=int, default=3,
                        help='ê²€ìƒ‰í•  ìœ ì‚¬ ì˜ˆì‹œ ê°œìˆ˜')
    args = parser.parse_args()

    evaluate_rag(args.ground_truth, args.n_examples)

"""
Level 1: ì •ëŸ‰ì  í‰ê°€ ì‹œìŠ¤í…œ
AI ì˜ˆì¸¡ ê²°ê³¼ì™€ Ground Truthë¥¼ ë¹„êµí•˜ì—¬ ì •í™•ë„ ì¸¡ì •
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import json
from analyzer import ReviewAnalyzer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

class Evaluator:
    def __init__(self, ground_truth_file='evaluation/evaluation_dataset.csv'):
        self.ground_truth_file = ground_truth_file
        self.analyzer = ReviewAnalyzer()

    def load_ground_truth(self):
        """Ground Truth ë°ì´í„° ë¡œë“œ"""
        df = pd.read_csv(self.ground_truth_file)

        # manual_labelì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        missing_mask = df['manual_label'].isna() | (df['manual_label'].str.strip() == '')
        if missing_mask.any():
            missing_count = missing_mask.sum()
            print(f"âš ï¸  ê²½ê³ : {missing_count}ê°œ ë¦¬ë·°ê°€ ì•„ì§ ë¼ë²¨ë§ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ëª¨ë“  ë¦¬ë·°ë¥¼ ë¼ë²¨ë§í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None

        return df

    def predict_categories(self, reviews_text_list):
        """AIë¡œ ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡"""
        print("\nğŸ¤– AI ì˜ˆì¸¡ ì¤‘...")
        categorization = self.analyzer.categorize_issues(reviews_text_list)

        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ë·° ìˆœì„œëŒ€ë¡œ ì •ë ¬
        predictions = {}
        for item in categorization.get('categories', []):
            review_num = item['review_number'] - 1  # 0-indexed
            predictions[review_num] = item['category']

        # ìˆœì„œëŒ€ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        pred_list = [predictions.get(i, 'other') for i in range(len(reviews_text_list))]
        return pred_list

    def calculate_metrics(self, y_true, y_pred):
        """ì •í™•ë„ ì§€í‘œ ê³„ì‚°"""
        # Overall metrics
        accuracy = accuracy_score(y_true, y_pred)

        # Per-class metrics
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average='weighted', zero_division=0
        )

        # Per-class detailed metrics
        precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0, labels=sorted(set(y_true))
        )

        metrics = {
            'accuracy': round(accuracy, 4),
            'precision_weighted': round(precision, 4),
            'recall_weighted': round(recall, 4),
            'f1_weighted': round(f1, 4),
            'per_class_metrics': {}
        }

        for idx, label in enumerate(sorted(set(y_true))):
            metrics['per_class_metrics'][label] = {
                'precision': round(precision_per_class[idx], 4),
                'recall': round(recall_per_class[idx], 4),
                'f1': round(f1_per_class[idx], 4),
                'support': int(support_per_class[idx])
            }

        return metrics

    def create_confusion_matrix(self, y_true, y_pred, output_file='results/confusion_matrix.png'):
        """Confusion Matrix ìƒì„±"""
        os.makedirs('results', exist_ok=True)

        labels = sorted(set(y_true) | set(y_pred))
        cm = confusion_matrix(y_true, y_pred, labels=labels)

        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels)
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"\nğŸ“Š Confusion Matrix ì €ì¥: {output_file}")
        plt.close()

        return cm, labels

    def analyze_errors(self, df, y_true, y_pred):
        """ì—ëŸ¬ ì¼€ì´ìŠ¤ ë¶„ì„"""
        errors = []
        for idx, (true_label, pred_label) in enumerate(zip(y_true, y_pred, strict=True)):
            if true_label != pred_label:
                errors.append({
                    'review_id': df.iloc[idx]['review_id'],
                    'review_text': df.iloc[idx]['review_text'],
                    'true_label': true_label,
                    'predicted_label': pred_label,
                    'rating': df.iloc[idx]['rating']
                })

        return errors

    def save_results(self, metrics, errors, mode='baseline'):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs('results', exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ë©”íŠ¸ë¦­ìŠ¤ ì €ì¥
        metrics_file = f'results/{mode}_metrics_{timestamp}.json'
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ë©”íŠ¸ë¦­ìŠ¤ ì €ì¥: {metrics_file}")

        # ì—ëŸ¬ ì¼€ì´ìŠ¤ ì €ì¥
        if errors:
            errors_file = f'results/{mode}_errors_{timestamp}.json'
            with open(errors_file, 'w', encoding='utf-8') as f:
                json.dump(errors, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ì—ëŸ¬ ì¼€ì´ìŠ¤ ì €ì¥: {errors_file}")

    def print_results(self, metrics, errors):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("  í‰ê°€ ê²°ê³¼")
        print("="*80)

        print("\nğŸ“Š Overall Metrics:")
        print(f"   Accuracy:  {metrics['accuracy']*100:.2f}%")
        print(f"   Precision: {metrics['precision_weighted']*100:.2f}%")
        print(f"   Recall:    {metrics['recall_weighted']*100:.2f}%")
        print(f"   F1 Score:  {metrics['f1_weighted']*100:.2f}%")

        print("\nğŸ“ˆ Per-Class Metrics:")
        print(f"{'Category':<25} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<8}")
        print("-" * 80)

        for category, class_metrics in metrics['per_class_metrics'].items():
            print(f"{category:<25} "
                  f"{class_metrics['precision']*100:>6.2f}%    "
                  f"{class_metrics['recall']*100:>6.2f}%    "
                  f"{class_metrics['f1']*100:>6.2f}%    "
                  f"{class_metrics['support']:>6}")

        if errors:
            print(f"\nâŒ ì´ {len(errors)}ê°œ ì—ëŸ¬ ì¼€ì´ìŠ¤")
            print("\n   ì—ëŸ¬ ì˜ˆì‹œ (ì²˜ìŒ 3ê°œ):")
            for i, error in enumerate(errors[:3], 1):
                print(f"\n   {i}. Review ID: {error['review_id']}")
                print(f"      Text: {error['review_text'][:100]}...")
                print(f"      True: {error['true_label']} â†’ Predicted: {error['predicted_label']}")

    def evaluate(self, mode='baseline'):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("="*80)
        print(f"  í‰ê°€ ì‹œì‘ (Mode: {mode})")
        print("="*80)

        # Ground Truth ë¡œë“œ
        print("\n1. Ground Truth ë¡œë”© ì¤‘...")
        df = self.load_ground_truth()
        if df is None:
            return

        print(f"   âœ“ {len(df)}ê°œ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ")

        # AI ì˜ˆì¸¡
        print("\n2. AI ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...")
        reviews = df['review_text'].tolist()
        y_pred = self.predict_categories(reviews)
        y_true = df['manual_label'].tolist()

        print("   âœ“ ì˜ˆì¸¡ ì™„ë£Œ")

        # ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        print("\n3. ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì¤‘...")
        metrics = self.calculate_metrics(y_true, y_pred)

        # Confusion Matrix ìƒì„±
        print("\n4. Confusion Matrix ìƒì„± ì¤‘...")
        self.create_confusion_matrix(y_true, y_pred, f'results/{mode}_confusion_matrix.png')

        # ì—ëŸ¬ ë¶„ì„
        print("\n5. ì—ëŸ¬ ì¼€ì´ìŠ¤ ë¶„ì„ ì¤‘...")
        errors = self.analyze_errors(df, y_true, y_pred)

        # ê²°ê³¼ ì €ì¥
        print("\n6. ê²°ê³¼ ì €ì¥ ì¤‘...")
        self.save_results(metrics, errors, mode)

        # ê²°ê³¼ ì¶œë ¥
        self.print_results(metrics, errors)

        print("\n" + "="*80)
        print("  í‰ê°€ ì™„ë£Œ!")
        print("="*80)

        return metrics, errors


def main():
    import argparse

    parser = argparse.ArgumentParser(description='ë¦¬ë·° ë¶„ì„ ì‹œìŠ¤í…œ í‰ê°€')
    parser.add_argument('--mode', type=str, default='baseline',
                        help='í‰ê°€ ëª¨ë“œ (baseline, improved, final)')
    args = parser.parse_args()

    evaluator = Evaluator()
    evaluator.evaluate(mode=args.mode)


if __name__ == "__main__":
    # sklearn, matplotlib, seaborn íŒ¨í‚¤ì§€ í™•ì¸
    try:
        import sklearn
        import matplotlib
        import seaborn
    except ImportError as e:
        print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
        print("\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install scikit-learn matplotlib seaborn")
        sys.exit(1)

    main()

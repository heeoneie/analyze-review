"""
Day 7: Fine-tuned ëª¨ë¸ í‰ê°€
Fine-tuningí•œ ëª¨ë¸ì˜ ì„±ëŠ¥ ì¸¡ì •
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import json
from openai import OpenAI
import config
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

class FinetunedEvaluator:
    def __init__(self, model_name):
        """
        Args:
            model_name: Fine-tuned ëª¨ë¸ ì´ë¦„ (ì˜ˆ: ft:gpt-4o-mini:custom:review-classifier:xxx)
        """
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.model_name = model_name

    def categorize_single(self, review_text):
        """ë‹¨ì¼ ë¦¬ë·° ë¶„ë¥˜"""
        system_prompt = """You are an expert at analyzing e-commerce customer reviews and categorizing their primary complaints.

Categories:
- delivery_delay: Shipping or delivery issues
- wrong_item: Received incorrect product
- poor_quality: Product quality problems
- damaged_packaging: Damaged package or product
- size_issue: Size-related issues
- missing_parts: Missing parts or accessories
- not_as_described: Product doesn't match description
- customer_service: Customer service issues
- price_issue: Price-related complaints
- other: Cannot be categorized

Return a JSON object only with this schema:
{"category": "<one of the categories above>"}"""

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Categorize this review: {review_text}"}
            ],
            temperature=0.0,  # Deterministic
            response_format={"type": "json_object"},
        )

        raw_content = response.choices[0].message.content
        try:
            parsed = json.loads(raw_content)
        except json.JSONDecodeError as exc:
            raise ValueError(f"Invalid JSON response: {raw_content}") from exc

        if not isinstance(parsed, dict) or "category" not in parsed:
            raise ValueError(f"Unexpected response format: {parsed}")

        category = parsed.get("category")
        if not isinstance(category, str) or not category.strip():
            raise ValueError(f"Invalid category value: {category}")

        return category.strip()

    def evaluate(self, ground_truth_file):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("="*80)
        print(f"  Fine-tuned ëª¨ë¸ í‰ê°€: {self.model_name}")
        print("="*80 + "\n")

        # Ground Truth ë¡œë“œ
        print("ğŸ“‚ Ground Truth ë¡œë”©...")
        df = pd.read_csv(ground_truth_file)
        df = df[df['manual_label'].notna()]

        print(f"   âœ“ {len(df)}ê°œ ë¦¬ë·° ë¡œë“œ\n")

        # ì˜ˆì¸¡ ì‹¤í–‰
        print("ğŸ¤– Fine-tuned ëª¨ë¸ë¡œ ì˜ˆì¸¡ ì¤‘...")
        predictions = []

        for i, (_, row) in enumerate(df.iterrows(), start=1):
            print(f"   [{i}/{len(df)}] ì˜ˆì¸¡ ì¤‘...", end='\r')

            try:
                pred = self.categorize_single(row['review_text'])
                predictions.append(pred)
            except Exception as e:
                print(f"\n   âš ï¸  ì—ëŸ¬ (Review {i}): {e}")
                predictions.append('other')

        print(f"\n   âœ“ ì™„ë£Œ!\n")

        # í‰ê°€
        y_true = df['manual_label'].tolist()
        y_pred = predictions

        # ë©”íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='weighted', zero_division=0
        )

        # ê²°ê³¼ ì¶œë ¥
        print("="*80)
        print("  í‰ê°€ ê²°ê³¼")
        print("="*80 + "\n")

        print(f"ğŸ“Š Overall Metrics:")
        print(f"   Accuracy:  {accuracy*100:.2f}%")
        print(f"   Precision: {precision*100:.2f}%")
        print(f"   Recall:    {recall*100:.2f}%")
        print(f"   F1 Score:  {f1*100:.2f}%\n")

        # ì—ëŸ¬ ë¶„ì„
        errors = []
        for i, (true, pred) in enumerate(zip(y_true, y_pred)):
            if true != pred:
                errors.append({
                    'review': df.iloc[i]['review_text'],
                    'true': true,
                    'predicted': pred
                })

        if errors:
            print(f"âŒ ì—ëŸ¬ ì¼€ì´ìŠ¤: {len(errors)}ê°œ\n")
            print("   ì˜ˆì‹œ (ì²˜ìŒ 3ê°œ):")
            for i, error in enumerate(errors[:3], 1):
                print(f"\n   {i}. {error['review'][:100]}...")
                print(f"      True: {error['true']} â†’ Predicted: {error['predicted']}")

        # ê²°ê³¼ ì €ì¥
        results = {
            'model': self.model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'total_samples': len(df),
            'errors': len(errors)
        }

        os.makedirs('results', exist_ok=True)
        output_file = 'results/finetuned_evaluation.json'

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")

        return results


def compare_models(base_results_file, finetuned_results_file):
    """Base ëª¨ë¸ê³¼ Fine-tuned ëª¨ë¸ ë¹„êµ"""
    print("\n" + "="*80)
    print("  ëª¨ë¸ ë¹„êµ")
    print("="*80 + "\n")

    with open(base_results_file, 'r') as f:
        base_results = json.load(f)

    with open(finetuned_results_file, 'r') as f:
        ft_results = json.load(f)

    print(f"{'Metric':<15} {'Base Model':<15} {'Fine-tuned':<15} {'Improvement':<15}")
    print("-" * 80)

    metrics = ['accuracy', 'precision', 'recall', 'f1']
    for metric in metrics:
        base_val = base_results.get(metric, 0) * 100
        ft_val = ft_results.get(metric, 0) * 100
        improvement = ft_val - base_val

        print(f"{metric.capitalize():<15} {base_val:>6.2f}%        "
              f"{ft_val:>6.2f}%        "
              f"{improvement:>+6.2f}%")

    print("\nğŸ’¡ ê²°ë¡ :")
    if ft_results['accuracy'] > base_results['accuracy']:
        improvement_pct = (ft_results['accuracy'] - base_results['accuracy']) * 100
        print(f"   Fine-tuningìœ¼ë¡œ {improvement_pct:.1f}% ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("   Fine-tuning íš¨ê³¼ê°€ ë¯¸ë¯¸í•˜ê±°ë‚˜ ë¶€ì •ì ì…ë‹ˆë‹¤.")
        print("   â†’ í•™ìŠµ ë°ì´í„° ì¶”ê°€ ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Fine-tuned ëª¨ë¸ í‰ê°€')
    parser.add_argument('--model', type=str, required=True,
                        help='Fine-tuned ëª¨ë¸ ì´ë¦„ (ì˜ˆ: ft:gpt-4o-mini:custom:review-classifier:xxx)')
    parser.add_argument('--ground-truth', type=str,
                        default='evaluation/evaluation_dataset.csv',
                        help='Ground Truth CSV íŒŒì¼')
    parser.add_argument('--compare', type=str,
                        help='ë¹„êµí•  Base ëª¨ë¸ ê²°ê³¼ íŒŒì¼ (ì˜ˆ: results/baseline_metrics.json)')
    args = parser.parse_args()

    # í‰ê°€ ì‹¤í–‰
    evaluator = FinetunedEvaluator(args.model)
    results = evaluator.evaluate(args.ground_truth)

    # ë¹„êµ (ì„ íƒì‚¬í•­)
    if args.compare:
        if os.path.exists(args.compare):
            compare_models(args.compare, 'results/finetuned_evaluation.json')
        else:
            print(f"\nâš ï¸  ë¹„êµ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.compare}")


if __name__ == "__main__":
    main()

"""
Day 7: Fine-tuning í•™ìŠµ ë°ì´í„° ì¤€ë¹„
OpenAI Fine-tuning API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
"""

import argparse
import json
import os

import pandas as pd
from utils.review_categories import CATEGORIES_BULLETS_FINETUNE

CATEGORIES_DESCRIPTION = {
    'delivery_delay': 'Shipping or delivery took too long',
    'wrong_item': 'Received incorrect product or wrong item',
    'poor_quality': 'Product quality is poor or defective',
    'damaged_packaging': 'Package or product was damaged during shipping',
    'size_issue': 'Product size does not fit or is incorrect',
    'missing_parts': 'Parts or accessories are missing from the package',
    'not_as_described': 'Product does not match description or photos',
    'customer_service': 'Issues with customer service response or support',
    'price_issue': 'Price-related complaints or concerns',
    'other': 'Cannot be categorized into above categories'
}

class TrainingDataPreparator:
    def __init__(self, ground_truth_file):
        self.ground_truth_file = ground_truth_file
        self.training_data = []
        self.validation_data = []

    def load_ground_truth(self):
        """Ground Truth ?????????????"""
        print(f"??? Ground Truth ??????: {self.ground_truth_file}")

        df = pd.read_csv(self.ground_truth_file)

        required_columns = {"manual_label", "review_text"}
        missing_columns = required_columns - set(df.columns)
        if missing_columns:
            raise ValueError(f"Missing required columns: {sorted(missing_columns)}")

        initial_count = len(df)
        allowed_categories = set(CATEGORIES_DESCRIPTION.keys())

        df = df[df["manual_label"].notna()].copy()
        df["manual_label"] = df["manual_label"].astype(str).str.strip().str.lower()
        df = df[df["manual_label"] != ""]
        df = df[df["manual_label"].isin(allowed_categories)]

        df = df[df["review_text"].notna()].copy()
        df["review_text"] = df["review_text"].astype(str).str.strip()
        df = df[df["review_text"] != ""]

        if "category" in df.columns:
            df = df[df["category"].notna()].copy()
            df["category"] = df["category"].astype(str).str.strip().str.lower()
            df = df[df["category"] != ""]
            df = df[df["category"].isin(allowed_categories)]

        before_dedup = len(df)
        df = df.drop_duplicates(subset=["review_text"])
        deduped_count = before_dedup - len(df)
        removed_total = initial_count - len(df)

        print(
            f"   ??{len(df)}?????????????? ?????? ?????? "
            f"(?????? {removed_total}??? / ?????? {deduped_count}???)"
        )

        return df

    def create_training_example(self, review_text, category):
        """ë‹¨ì¼ í•™ìŠµ ì˜ˆì‹œ ìƒì„± (OpenAI Format)"""
        system_prompt = (
            "You are an expert at analyzing e-commerce customer reviews and "
            "categorizing their primary complaints.\n\n"
            "Categories:\n"
            f"{CATEGORIES_BULLETS_FINETUNE}\n"
            "Respond with ONLY the category name, nothing else."
        )

        example = {
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": f"Categorize this review: {review_text}"
                },
                {
                    "role": "assistant",
                    "content": category
                }
            ]
        }

        return example

    def split_train_validation(self, df, validation_ratio=0.2):
        """Train/Validation ë¶„ë¦¬"""
        # ëœë¤ ì…”í”Œ
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)

        # ë¶„ë¦¬
        split_idx = int(len(df) * (1 - validation_ratio))
        train_df = df[:split_idx]
        val_df = df[split_idx:]

        print("\nğŸ“Š ë°ì´í„° ë¶„ë¦¬:")
        print(f"   Train: {len(train_df)}ê°œ")
        print(f"   Validation: {len(val_df)}ê°œ")

        return train_df, val_df

    def prepare_data(self, output_dir='fine_tuning'):
        """ì „ì²´ ë°ì´í„° ì¤€ë¹„"""
        os.makedirs(output_dir, exist_ok=True)

        print("="*80)
        print("  Fine-tuning í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
        print("="*80 + "\n")

        # Ground Truth ë¡œë“œ
        df = self.load_ground_truth()

        # Train/Validation ë¶„ë¦¬
        train_df, val_df = self.split_train_validation(df)

        # Train ë°ì´í„° ìƒì„±
        print("\nâš™ï¸  Train ë°ì´í„° ìƒì„± ì¤‘...")
        for _, row in train_df.iterrows():
            example = self.create_training_example(
                row['review_text'],
                row['manual_label']
            )
            self.training_data.append(example)

        # Validation ë°ì´í„° ìƒì„±
        print("âš™ï¸  Validation ë°ì´í„° ìƒì„± ì¤‘...")
        for _, row in val_df.iterrows():
            example = self.create_training_example(
                row['review_text'],
                row['manual_label']
            )
            self.validation_data.append(example)

        # JSONL íŒŒì¼ë¡œ ì €ì¥
        train_file = os.path.join(output_dir, 'training_data.jsonl')
        val_file = os.path.join(output_dir, 'validation_data.jsonl')

        print("\nğŸ’¾ ì €ì¥ ì¤‘...")
        with open(train_file, 'w', encoding='utf-8') as f:
            for example in self.training_data:
                f.write(json.dumps(example, ensure_ascii=False) + '\n')

        with open(val_file, 'w', encoding='utf-8') as f:
            for example in self.validation_data:
                f.write(json.dumps(example, ensure_ascii=False) + '\n')

        print(f"   âœ“ Train: {train_file}")
        print(f"   âœ“ Validation: {val_file}")

        # í†µê³„ ì •ë³´
        self.print_statistics(train_df, val_df)

        return train_file, val_file

    def print_statistics(self, train_df, val_df):
        """ë°ì´í„° í†µê³„ ì¶œë ¥"""
        print("\n" + "="*80)
        print("  ë°ì´í„° í†µê³„")
        print("="*80)

        print("\nğŸ“Š Train ë°ì´í„° ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        train_counts = train_df['manual_label'].value_counts()
        for category, count in train_counts.items():
            percentage = (count / len(train_df)) * 100
            print(f"   {category:<25}: {count:>3}ê°œ ({percentage:>5.1f}%)")

        print(f"\n   ì´ Train ìƒ˜í”Œ: {len(train_df)}ê°œ")
        print(f"   ì´ Validation ìƒ˜í”Œ: {len(val_df)}ê°œ")


def main():
    parser = argparse.ArgumentParser(description='Fine-tuning í•™ìŠµ ë°ì´í„° ì¤€ë¹„')
    parser.add_argument('--ground-truth', type=str,
                        default='evaluation/evaluation_dataset.csv',
                        help='Ground Truth CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', type=str,
                        default='fine_tuning',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    args = parser.parse_args()

    if not os.path.exists(args.ground_truth):
        print(f"\nâŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.ground_truth}")
        print("ë¨¼ì € Ground Truthë¥¼ ì¤€ë¹„í•˜ì„¸ìš”.")
        return

    preparator = TrainingDataPreparator(args.ground_truth)
    train_file, val_file = preparator.prepare_data(args.output)

    print("\n" + "="*80)
    print("  âœ… ì¤€ë¹„ ì™„ë£Œ!")
    print("="*80)

    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. OpenAI CLI ì„¤ì¹˜:")
    print("   pip install --upgrade openai")

    print("\n2. Fine-tuning ì‹œì‘:")
    print("   openai api fine_tuning.jobs.create \\")
    print(f"     -t {train_file} \\")
    print(f"     -v {val_file} \\")
    print("     -m gpt-4o-mini-2024-07-18 \\")
    print("     --suffix \"review-classifier\"")

    print("\n3. ì§„í–‰ ìƒí™© í™•ì¸:")
    print("   openai api fine_tuning.jobs.list")

    print("\n4. ì™„ë£Œ í›„ ëª¨ë¸ ì‚¬ìš©:")
    print("   ëª¨ë¸ ì´ë¦„: ft:gpt-4o-mini:custom:review-classifier:xxx")


if __name__ == "__main__":
    main()

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Business Direction

**OntoReview** â€” ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ê¸€ë¡œë²Œ ë¦¬ìŠ¤í¬ ì¸í…”ë¦¬ì „ìŠ¤ í”Œë«í¼ ("Palantir for Reputation")

- **ì›ë˜**: ì´ì»¤ë¨¸ìŠ¤ ë¦¬ë·° ê´€ë¦¬ PoC â†’ **ì „í™˜ í›„**: ë¬´ì‚¬ê³  ì¦ëª… B2B SaaS
- **ì°¨ë³„ì **: ë‹¨ìˆœ ê°ì„± í†µê³„ê°€ ì•„ë‹Œ ì˜¨í†¨ë¡œì§€ + Knowledge Graph + AIë¡œ ë¦¬ìŠ¤í¬ ì¸ê³¼ê´€ê³„ ì¶”ë¡ 
- **íƒ€ê²Ÿ**: ë³‘ì›/ê¸ˆìœµ/í•€í…Œí¬(1í‹°ì–´), ê¸€ë¡œë²Œ ë¸Œëœë“œ(2í‹°ì–´), ê²Œì„/ì—”í„°(3í‹°ì–´)
- **ë°ì´í„° ì „ëµ**: í˜„ì¬ Mock â†’ ê³µì‹ API ìš°ì„  (YouTube â†’ Reddit â†’ Naver ìˆœì„œ, ë´‡ í¬ë¡¤ë§ ì§€ì–‘)
- **6ê°œ íƒ­ ì¤‘ 1ê°œ êµ¬í˜„**: Risk Intelligenceë§Œ ì™„ì„±, ë‚˜ë¨¸ì§€ 5ê°œ Coming Soon
  - âœ… Risk Intelligence (ì˜¨í†¨ë¡œì§€ ê·¸ë˜í”„ + ì»´í”Œë¼ì´ì–¸ìŠ¤ + íšŒì˜ ì•ˆê±´)
  - ğŸ”’ Risk Response Playbook / Agent Communication Setup / Domain Ontology Studio / Global Compliance Tracker / Trust & Safety Audit
- **ì„¤ê³„ ì›ì¹™**: ë‹¤êµ­ì–´ ì²˜ìŒë¶€í„°(í•œ/ì˜), ê¸€ë¡œë²Œ íƒ€ê²Ÿ, ê¸°ìˆ  2ê°œ ì´ìƒ ìœµí•©, í”„ë¦¬ë¯¸ì—„ í¬ì§€ì…”ë‹

## Project Overview

ì´ì»¤ë¨¸ìŠ¤ ë¦¬ë·° ë¶„ì„ PoCë¡œ ì‹œì‘í•˜ì—¬ ë¦¬ìŠ¤í¬ ì¸í…”ë¦¬ì „ìŠ¤ í”Œë«í¼ìœ¼ë¡œ ë°œì „. LLM(GPT-4o-mini)ìœ¼ë¡œ ë¦¬ë·°/ì†Œì…œ ë°ì´í„°ë¥¼ ë¶„ì„í•´ ë¦¬ìŠ¤í¬ ì¸ê³¼ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ê³ , ì»´í”Œë¼ì´ì–¸ìŠ¤ ë³´ê³ ì„œ ë° ê¸´ê¸‰ íšŒì˜ ì•ˆê±´ì„ ìë™ ìƒì„±í•œë‹¤.

## Project Structure

```
review-dashboard/
â”œâ”€â”€ core/                    # Core AI analysis package
â”‚   â”œâ”€â”€ config.py            # Environment & analysis configuration
â”‚   â”œâ”€â”€ analyzer.py          # LLM-powered review categorization
â”‚   â”œâ”€â”€ data_loader.py       # Data loading (Kaggle, CSV)
â”‚   â”œâ”€â”€ report_utils.py      # Report formatting
â”‚   â”œâ”€â”€ utils/               # Shared utilities
â”‚   â”‚   â”œâ”€â”€ json_utils.py
â”‚   â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”‚   â”œâ”€â”€ prompt_templates.py
â”‚   â”‚   â”œâ”€â”€ review_categories.py
â”‚   â”‚   â”œâ”€â”€ cli_helpers.py
â”‚   â”‚   â””â”€â”€ analysis_workflow.py
â”‚   â””â”€â”€ experiments/         # Experimental features
â”‚       â”œâ”€â”€ multi_agent_analyzer.py
â”‚       â”œâ”€â”€ rag_system.py
â”‚       â”œâ”€â”€ evaluate.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ backend/                 # FastAPI server
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routers/             # API endpoints
â”‚   â””â”€â”€ services/            # Business logic & crawlers
â”œâ”€â”€ frontend/                # React + Vite dashboard
â”‚   â””â”€â”€ src/
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ main.py                  # CLI entry point (Kaggle)
â””â”€â”€ analyze_csv.py           # CLI entry point (custom CSV)
```

## Development Commands

### Setup
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
# Create .env file with: OPENAI_API_KEY=your_key_here
```

### Running the Web App
```bash
# Backend
uvicorn backend.main:app --reload

# Frontend
cd frontend && npm run dev
```

### Running CLI Analysis
```bash
python main.py                              # Kaggle dataset
python analyze_csv.py <path_to_csv_file>    # Custom CSV
```

### Testing
```bash
# Python tests (pytest)
pytest -v

# Frontend tests (vitest)
cd frontend && npm test
```

## Git Branch & Commit Convention

### Branch Naming
í•­ìƒ `main`ì—ì„œ ìƒˆ ë¸Œëœì¹˜ë¥¼ ë§Œë“¤ì–´ ì‘ì—…í•˜ê³ , mainì— ì§ì ‘ ì»¤ë°‹í•˜ì§€ ì•ŠëŠ”ë‹¤.

| Prefix | ìš©ë„ | ì˜ˆì‹œ |
|--------|------|------|
| `feat/` | ìƒˆ ê¸°ëŠ¥ ì¶”ê°€ | `feat/add-crawling` |
| `fix/` | ë²„ê·¸ ìˆ˜ì • | `fix/nan-serialization` |
| `refactor/` | ë¦¬íŒ©í„°ë§ (ë™ì‘ ë³€ê²½ ì—†ìŒ) | `refactor/folder-structure` |
| `test/` | í…ŒìŠ¤íŠ¸ ì¶”ê°€/ìˆ˜ì •ë§Œ | `test/add-unit-tests` |
| `docs/` | ë¬¸ì„œë§Œ ë³€ê²½ | `docs/update-readme` |
| `chore/` | ë¹Œë“œ/ì„¤ì •/ì˜ì¡´ì„± | `chore/upgrade-deps` |

### Commit Message
Conventional Commits í˜•ì‹ì„ ë”°ë¥¸ë‹¤:
```
<type>: <í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´ ìš”ì•½>
```
- type: `feat`, `fix`, `refactor`, `test`, `docs`, `chore`
- í•œ ì¤„ ìš”ì•½, í•„ìš”ì‹œ ë³¸ë¬¸ì— ìƒì„¸ ì„¤ëª…

### Workflow
1. `main`ì—ì„œ ë¸Œëœì¹˜ ìƒì„±: `git checkout -b <prefix>/<ì„¤ëª…>`
2. ì‘ì—… í›„ ì»¤ë°‹ & í‘¸ì‹œ
3. GitHub PR ìƒì„± â†’ mainìœ¼ë¡œ ë¨¸ì§€

## Architecture

### Module Responsibilities

**core/config.py**
- Environment configuration (OpenAI API key, paths)
- Analysis parameters: `NEGATIVE_RATING_THRESHOLD` (default: 3), `RECENT_PERIOD_DAYS` (30), `COMPARISON_PERIOD_DAYS` (60)
- LLM settings: `LLM_MODEL` (gpt-4o-mini), `LLM_TEMPERATURE` (0.3)

**core/data_loader.py** (`DataLoader` class)
- `load_reviews()`: Downloads and processes Kaggle Olist dataset
- `load_custom_csv()`: Processes custom CSV with Ratings/Reviews columns
- `filter_negative_reviews()`: Extracts reviews <= threshold rating
- `split_by_period()`: Divides data into recent/comparison periods

**core/analyzer.py** (`ReviewAnalyzer` class)
- `categorize_issues()`: Batches reviews and prompts GPT to categorize into issue types
- `get_top_issues()`: Counts category frequencies and extracts top N
- `detect_emerging_issues()`: Compares recent vs comparison period counts
- `generate_action_plan()`: Generates Korean-language actionable recommendations

### Key Design Patterns

**LLM Integration:** All OpenAI calls use `response_format={"type": "json_object"}` for structured output. Temperature set to 0.3.

**Time-Series Comparison:** Dataset max date is reference point. Recent period vs comparison period enables trend detection.

**Sampling Strategy:** `categorize_issues()` caps at 200 reviews per LLM call to avoid token limits.

## Configuration

Edit `core/config.py` to adjust:
- `NEGATIVE_RATING_THRESHOLD`: Rating cutoff for negative reviews (default: 3)
- `RECENT_PERIOD_DAYS`: Time window for "recent" analysis (default: 30)
- `COMPARISON_PERIOD_DAYS`: Total time window including comparison baseline (default: 60)
- `LLM_MODEL`: OpenAI model to use (default: "gpt-4o-mini")
- `LLM_TEMPERATURE`: Response randomness (default: 0.3)

## Important Notes

**API Costs:** Each run calls OpenAI API 3+ times. Default 200-sample batches with gpt-4o-mini cost ~$0.10-0.30 per run.

**Output Language:** Action plan recommendations are generated in Korean. Issue categories use English snake_case.

**Custom CSV:** Requires exact column names `Ratings` (numeric) and `Reviews` (text). Synthetic timestamps are generated for trend detection.
